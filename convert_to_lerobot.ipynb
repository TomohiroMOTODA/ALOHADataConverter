{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ab22d12-3b34-47b8-a385-e1e27fb0d2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import natsort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c78e95e-5b1a-40fc-bbd0-2f5f24f3dcdf",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "148e5f3a-844f-43a8-8937-3b7f2bcd5009",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = Path(\"./ALOHATaskCompressedData/\")  # ← CHANGE THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c824f639-3f7b-4509-b92f-7f6726228b26",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ALOHATaskCompressedData'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset_folder \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparent_dir\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(dataset_folder)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Using natsort to naturally sort the episode files\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/pathlib.py:1017\u001b[0m, in \u001b[0;36mPath.iterdir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21miterdir\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Iterate over the files in this directory.  Does not yield any\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;124;03m    result for the special paths '.' and '..'.\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1017\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1018\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m}:\n\u001b[1;32m   1019\u001b[0m             \u001b[38;5;66;03m# Yielding a path object for these makes little sense\u001b[39;00m\n\u001b[1;32m   1020\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ALOHATaskCompressedData'"
     ]
    }
   ],
   "source": [
    "for dataset_folder in sorted(parent_dir.iterdir()):\n",
    "    print(dataset_folder)\n",
    "\n",
    "    # Using natsort to naturally sort the episode files\n",
    "    sorted_files = natsort.natsorted(dataset_folder.glob(\"episode*.hdf5\"))\n",
    "\n",
    "    for hdf5_file in sorted_files:\n",
    "        print(hdf5_file)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306b771d-d0ab-461d-a74e-722695eea6f0",
   "metadata": {},
   "source": [
    "## Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abc4ac0e-980e-4304-b837-7598b2981fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle datasets inside the HDF5 file\n",
    "def extract_data(f, arrays):\n",
    "    for name, obj in f.items():  # Iterate over items in the root group\n",
    "        if isinstance(obj, h5py.Dataset):  # If it's a dataset (not a group)\n",
    "            array = obj[()]  # Extract the data from the dataset\n",
    "            arrays[name] = array  # Store it in the arrays dictionary\n",
    "        elif isinstance(obj, h5py.Group):  # If it's a group, recurse into it\n",
    "            for sub_name, sub_obj in obj.items():\n",
    "                if isinstance(sub_obj, h5py.Dataset):\n",
    "                    array = sub_obj[()]\n",
    "                    arrays[f\"{name}/{sub_name}\"] = array  # Store the data with full path as key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba1decc-0385-4348-ad38-b297668cddc2",
   "metadata": {},
   "source": [
    "## PROCESS EACH DATASET FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "125654fa-0a90-4155-a752-a815c3d1ad8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "fps = 30\n",
    "task_string = \"default task\"\n",
    "task_index = 0\n",
    "frame_time_interval = 0.1\n",
    "episode_index = 0\n",
    "frame_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15a12399-3fa8-48e5-95e4-92e64e858934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Processing: apply_tape_closed_box\n",
      "✅ Done: apply_tape_closed_box → 100 episodes.\n",
      "\n",
      "📦 Processing: pick_scraper_from_rack\n",
      "✅ Done: pick_scraper_from_rack → 50 episodes.\n",
      "\n",
      "📦 Processing: usb_mark_003\n",
      "✅ Done: usb_mark_003 → 50 episodes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Process each dataset folder\n",
    "for dataset_folder in sorted(parent_dir.iterdir()):\n",
    "    if dataset_folder.is_dir():  \n",
    "        print(f\"📦 Processing: {dataset_folder.name}\")\n",
    "\n",
    "        # Create necessary output directories specific to this dataset\n",
    "        out_dir = Path(f\"./lerobot_dataset/{dataset_folder.name}\")  \n",
    "        (out_dir / \"data/chunk-000\").mkdir(parents=True, exist_ok=True)\n",
    "        (out_dir / \"meta\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        episodes_meta = []\n",
    "        total_frames = 0\n",
    "        episode_count = episode_index  # Start with the provided episode_index\n",
    "\n",
    "        # Process each .h5 file in the dataset folder\n",
    "        for hdf5_file in sorted(dataset_folder.glob(\"episode*.hdf5\")):\n",
    "            with h5py.File(hdf5_file, 'r') as f:\n",
    "                # Dictionary to store arrays\n",
    "                arrays = {}\n",
    "\n",
    "                # Manually iterate over the HDF5 file items and extract data\n",
    "                extract_data(f, arrays)\n",
    "\n",
    "                # Extract observation and action components\n",
    "                obs_keys = [key for key in arrays.keys() if 'observations' in key]\n",
    "                action_keys = [key for key in arrays.keys() if 'action' in key]\n",
    "\n",
    "                # Stack all observation components into one array (axis=1 for horizontal stacking)\n",
    "                observation = np.concatenate([arrays[key] for key in obs_keys], axis=1).astype(np.float32)\n",
    "\n",
    "                # Stack all action components into one array\n",
    "                action = np.concatenate([arrays[key] for key in action_keys], axis=1).astype(np.float32)\n",
    "\n",
    "                T = action.shape[0]  # Number of time steps (same for observations and actions)\n",
    "                \n",
    "            # === Construct the new data dict ===\n",
    "            new_data = {\n",
    "                \"observation.state\": observation.tolist(),\n",
    "                \"action\": action.tolist(),\n",
    "                \"episode_index\": [episode_count] * T,\n",
    "                \"frame_index\": list(np.arange(frame_index, frame_index + T)),\n",
    "                \"timestamp\": list(np.arange(T) * frame_time_interval),  # Use the frame_time_interval parameter\n",
    "                \"next.done\": [False] * T,\n",
    "                \"index\": list(np.arange(total_frames, total_frames + T)),\n",
    "                \"task_index\": [task_index] * T,\n",
    "            }\n",
    "            new_data[\"next.done\"][-1] = True  # mark final frame as done\n",
    "\n",
    "            # === Save as .parquet ===\n",
    "            df = pd.DataFrame(new_data)\n",
    "            table = pa.Table.from_pandas(df)\n",
    "            pq.write_table(table, out_dir / f\"data/chunk-000/episode_{episode_count:06d}.parquet\")\n",
    "\n",
    "            episodes_meta.append({\n",
    "                \"episode_index\": episode_count,\n",
    "                \"length\": T,\n",
    "                \"tasks\": [task_string],\n",
    "            })\n",
    "\n",
    "            total_frames += T\n",
    "            frame_index += T  # Update frame_index for next set of frames\n",
    "            episode_count += 1  # Update episode_count for next episode\n",
    "\n",
    "        # === Save metadata ===\n",
    "        obs_dim = observation.shape[1]\n",
    "        act_dim = action.shape[1]\n",
    "\n",
    "        features = {\n",
    "            \"observation.state\": {\"dtype\": \"float32\", \"shape\": [obs_dim]},\n",
    "            \"action\": {\"dtype\": \"float32\", \"shape\": [act_dim]},\n",
    "            \"episode_index\": {\"dtype\": \"int64\", \"shape\": []},\n",
    "            \"frame_index\": {\"dtype\": \"int64\", \"shape\": []},\n",
    "            \"timestamp\": {\"dtype\": \"float64\", \"shape\": []},\n",
    "            \"next.done\": {\"dtype\": \"bool\", \"shape\": []},\n",
    "            \"index\": {\"dtype\": \"int64\", \"shape\": []},\n",
    "            \"task_index\": {\"dtype\": \"int64\", \"shape\": []}\n",
    "        }\n",
    "\n",
    "        info = {\n",
    "            \"fps\": fps,\n",
    "            \"codebase_version\": \"v2.1\",\n",
    "            \"robot_type\": None,\n",
    "            \"features\": features,\n",
    "            \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n",
    "            \"video_path\": None,\n",
    "            \"total_episodes\": episode_count,\n",
    "            \"total_frames\": total_frames,\n",
    "            \"chunks_size\": 1000,\n",
    "            \"total_chunks\": 1,\n",
    "            \"total_tasks\": 1\n",
    "        }\n",
    "\n",
    "        # Save the info JSON file with dataset-specific name\n",
    "        with open(out_dir / \"meta/info.json\", \"w\") as f:\n",
    "            json.dump(info, f, indent=2)\n",
    "\n",
    "        # Save the episodes metadata JSONL file with dataset-specific name\n",
    "        with open(out_dir / \"meta/episodes.jsonl\", \"w\") as f:\n",
    "            for ep in episodes_meta:\n",
    "                f.write(json.dumps(ep) + \"\\n\")\n",
    "\n",
    "        # Save the tasks metadata JSONL file with dataset-specific name\n",
    "        with open(out_dir / \"meta/tasks.jsonl\", \"w\") as f:\n",
    "            f.write(json.dumps({\"task_index\": task_index, \"task\": \"default task\"}) + \"\\n\")\n",
    "\n",
    "        print(f\"✅ Done: {dataset_folder.name} → {episode_count} episodes.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cd91ec-14c1-4254-bff5-3e066af729d3",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (muhammad-ts)",
   "language": "python",
   "name": "muhammad-ra"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
