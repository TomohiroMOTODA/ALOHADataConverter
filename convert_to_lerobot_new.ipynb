{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7ce505-ed9d-4311-a14e-8fe65aa9920d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "import natsort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651c7230-879c-49ee-9634-be916d33a1ab",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7231f7-0c2d-4557-b2cf-f11bd2aba8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = Path(\"../ALOHATaskCompressedData/\")  # ‚Üê CHANGE THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0695b0-8e65-45f0-be91-4fff93d823a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_folder in sorted(parent_dir.iterdir()):\n",
    "    print(dataset_folder)\n",
    "\n",
    "    # Using natsort to naturally sort the episode files\n",
    "    sorted_files = natsort.natsorted(dataset_folder.glob(\"episode*.hdf5\"))\n",
    "\n",
    "    for hdf5_file in sorted_files:\n",
    "        print(hdf5_file)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608deee2-4525-4981-8043-a4f6a471bf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONSTANTS\n",
    "CAMERA_NAMES = [\"cam_high\", \"cam_left_wrist\", \"cam_low\", \"cam_right_wrist\"]\n",
    "DCAMERA_NAMES = [\"dcam_high\", \"dcam_low\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8601082d-8660-4ff9-acf5-054905623f11",
   "metadata": {},
   "source": [
    "## Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a562c29-043e-4bff-95ef-033a28378ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle datasets inside the HDF5 file\n",
    "def extract_data(f, arrays):\n",
    "    for name, obj in f.items():  # Iterate over items in the root group\n",
    "        if isinstance(obj, h5py.Dataset):  # If it's a dataset (not a group)\n",
    "            array = obj[()]  # Extract the data from the dataset\n",
    "            arrays[name] = array  # Store it in the arrays dictionary\n",
    "        elif isinstance(obj, h5py.Group):  # If it's a group, recurse into it\n",
    "            for sub_name, sub_obj in obj.items():\n",
    "                if isinstance(sub_obj, h5py.Dataset):\n",
    "                    array = sub_obj[()]\n",
    "                    arrays[f\"{name}/{sub_name}\"] = array  # Store the data with full path as key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f312db-030f-49f7-89bc-1429a2bb3ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(f, arrays):\n",
    "    # Iterate over items in the root group (top-level groups and datasets)\n",
    "    for name, obj in f.items():\n",
    "        if isinstance(obj, h5py.Dataset):  # If it's a dataset (not a group)\n",
    "            # Extract data from the dataset\n",
    "            array = obj[()]\n",
    "            arrays[name] = array  # Store it in the arrays dictionary\n",
    "        elif isinstance(obj, h5py.Group):  # If it's a group, recurse into it\n",
    "            for sub_name, sub_obj in obj.items():\n",
    "                if isinstance(sub_obj, h5py.Dataset):  # If it's a dataset inside the group\n",
    "                    # Extract data from the sub-group dataset\n",
    "                    array = sub_obj[()]\n",
    "                    # Use full path (group name + dataset name) as the key in the dictionary\n",
    "                    full_name = f\"{name}/{sub_name}\"\n",
    "                    arrays[full_name] = array  # Store the dataset with full path as key\n",
    "                elif isinstance(sub_obj, h5py.Group):  # If it's a subgroup, recurse further\n",
    "                    # Recursively extract data from nested groups\n",
    "                    extract_data(sub_obj, arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da601208-35db-483c-9628-82ed7cb625b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dict_keys(arrays):\n",
    "    \n",
    "    old_keys = ['cam_high', 'cam_left_wrist', 'cam_low', 'cam_right_wrist']\n",
    "    updated_arrays = {}\n",
    "\n",
    "    for key, value in arrays.items():\n",
    "        # Check if the current key is in the old_keys list\n",
    "        if key in old_keys:\n",
    "            # Update key with the new path\n",
    "            new_key = f'observations/images/{key}'\n",
    "            updated_arrays[new_key] = value\n",
    "        else:\n",
    "            # Keep the existing key-value pair\n",
    "            updated_arrays[key] = value\n",
    "\n",
    "    return updated_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066b74a2-7b3c-4c24-a3d8-aa9d807188f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_compressed_hdf5(dataset_path):\n",
    "    if not os.path.isfile(dataset_path):\n",
    "        print(f'Dataset does not exist at \\n{dataset_path}\\n')\n",
    "        exit()\n",
    "    with h5py.File(dataset_path, 'r') as root:\n",
    "        is_sim = root.attrs['sim']\n",
    "        qpos = root['/observations/qpos'][()]\n",
    "        qvel = root['/observations/qvel'][()]\n",
    "        effort = root['/observations/effort'][()]\n",
    "        action = root['/action'][()]\n",
    "\n",
    "        image_dict = dict()\n",
    "        for cam_name in root[f'/observations/images/'].keys():\n",
    "            # decode images\n",
    "            emc_images = root[f'/observations/images/{cam_name}'][()]\n",
    "            image_dict[cam_name] = list()\n",
    "            for img in emc_images:\n",
    "                decompressed_image = cv2.imdecode(img , 1)\n",
    "                image_dict[cam_name].append(decompressed_image)\n",
    "    return is_sim, qpos, qvel, effort, action, image_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78190672-d408-4461-bd87-f102a57e76dd",
   "metadata": {},
   "source": [
    "## Dataset Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777d5e65-3644-4085-b7a8-8e2c16193822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "fps = 30\n",
    "task_string = \"default task\"\n",
    "task_index = 0\n",
    "frame_time_interval = 0.1\n",
    "episode_index = 0\n",
    "frame_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606598e1-594f-4a71-b5b0-a8da08628d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each dataset folder\n",
    "for dataset_folder in sorted(parent_dir.iterdir()):\n",
    "    if dataset_folder.is_dir():\n",
    "        print(f\"üì¶ Processing: {dataset_folder.name}\")\n",
    "        out_dir = Path(f\"./lerobot_dataset/{dataset_folder.name}\")\n",
    "        (out_dir / \"data/chunk-000\").mkdir(parents=True, exist_ok=True)\n",
    "        (out_dir / \"meta\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        episodes_meta = []\n",
    "        total_frames = 0\n",
    "        episode_count = episode_index\n",
    "\n",
    "        for hdf5_file in natsort.natsorted(dataset_folder.glob(\"episode*.hdf5\")):\n",
    "            \n",
    "            # Load data using provided function\n",
    "            is_sim, qpos, qvel, effort, action, image_dict = load_compressed_hdf5(hdf5_file)\n",
    "            T = action.shape[0]\n",
    "\n",
    "            # Prepare all arrays\n",
    "            arrays = {\n",
    "                \"observations.qpos\": qpos,\n",
    "                \"observations.qvel\": qvel,\n",
    "                \"observations.effort\": effort,\n",
    "                \"action\": action\n",
    "            }\n",
    "\n",
    "            # Add image vectors to the array dictionary\n",
    "            for cam_name in CAMERA_NAMES:\n",
    "                flattened_images = np.stack([img.flatten() for img in image_dict[cam_name]], axis=0)\n",
    "                arrays[f'observations.images.{cam_name}'] = flattened_images\n",
    "\n",
    "            # Observation keys (everything under 'observations.*')\n",
    "            obs_keys = [k for k in arrays if k.startswith('observations')]\n",
    "            act_keys = [k for k in arrays if k == 'action']\n",
    "\n",
    "            # Stack all observation components\n",
    "            observation = np.concatenate([arrays[k] for k in obs_keys], axis=1).astype(np.float32)\n",
    "            action = np.concatenate([arrays[k] for k in act_keys], axis=1).astype(np.float32)\n",
    "\n",
    "            # === Construct dataset\n",
    "            new_data = {\n",
    "                \"observation.state\": observation.tolist(),\n",
    "                \"action\": action.tolist(),\n",
    "                \"episode_index\": [episode_count] * T,\n",
    "                \"frame_index\": list(np.arange(frame_index, frame_index + T)),\n",
    "                \"timestamp\": list(np.arange(T) * frame_time_interval),\n",
    "                \"next.done\": [False] * T,\n",
    "                \"index\": list(np.arange(total_frames, total_frames + T)),\n",
    "                \"task_index\": [task_index] * T\n",
    "            }\n",
    "            new_data[\"next.done\"][-1] = True\n",
    "\n",
    "            print(\"SAVE!\")\n",
    "            # Save to parquet\n",
    "            df = pd.DataFrame(new_data)\n",
    "            table = pa.Table.from_pandas(df)\n",
    "            pq.write_table(table, out_dir / f\"data/chunk-000/episode_{episode_count:06d}.parquet\")\n",
    "\n",
    "            episodes_meta.append({\n",
    "                \"episode_index\": episode_count,\n",
    "                \"length\": T,\n",
    "                \"tasks\": [task_string]\n",
    "            })\n",
    "\n",
    "            total_frames += T\n",
    "            frame_index += T\n",
    "            episode_count += 1\n",
    "\n",
    "            break\n",
    "\n",
    "        # === Save metadata\n",
    "        obs_dim = observation.shape[1]\n",
    "        act_dim = action.shape[1]\n",
    "\n",
    "        features = {\n",
    "            \"observation.state\": {\"dtype\": \"float32\", \"shape\": [obs_dim]},\n",
    "            \"action\": {\"dtype\": \"float32\", \"shape\": [act_dim]},\n",
    "            \"episode_index\": {\"dtype\": \"int64\", \"shape\": []},\n",
    "            \"frame_index\": {\"dtype\": \"int64\", \"shape\": []},\n",
    "            \"timestamp\": {\"dtype\": \"float64\", \"shape\": []},\n",
    "            \"next.done\": {\"dtype\": \"bool\", \"shape\": []},\n",
    "            \"index\": {\"dtype\": \"int64\", \"shape\": []},\n",
    "            \"task_index\": {\"dtype\": \"int64\", \"shape\": []}\n",
    "        }\n",
    "\n",
    "        info = {\n",
    "            \"fps\": fps,\n",
    "            \"codebase_version\": \"v2.1\",\n",
    "            \"robot_type\": None,\n",
    "            \"features\": features,\n",
    "            \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n",
    "            \"video_path\": None,\n",
    "            \"total_episodes\": episode_count,\n",
    "            \"total_frames\": total_frames,\n",
    "            \"chunks_size\": 1000,\n",
    "            \"total_chunks\": 1,\n",
    "            \"total_tasks\": 1\n",
    "        }\n",
    "\n",
    "        # Save meta files\n",
    "        with open(out_dir / \"meta/info.json\", \"w\") as f:\n",
    "            json.dump(info, f, indent=2)\n",
    "\n",
    "        with open(out_dir / \"meta/episodes.jsonl\", \"w\") as f:\n",
    "            for ep in episodes_meta:\n",
    "                f.write(json.dumps(ep) + \"\\n\")\n",
    "\n",
    "        with open(out_dir / \"meta/tasks.jsonl\", \"w\") as f:\n",
    "            f.write(json.dumps({\"task_index\": task_index, \"task\": task_string}) + \"\\n\")\n",
    "\n",
    "        print(f\"‚úÖ Done: {dataset_folder.name} ‚Üí {episode_count} episodes.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9913ac87-5561-45ed-b7b0-1990c15fa490",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
