{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5c8c6b6-970c-4ed3-8981-92dc1d59e3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "import natsort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70b2055-bd1c-4c8d-9a7b-9053e9e2b98a",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4046cafe-dd48-4290-926d-6375b543eee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = Path(\"../ALOHATaskCompressedData/\")  # ← CHANGE THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25834c42-6963-4d33-a1dd-0af0eb96e435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\ALOHATaskCompressedData\\apply_tape_closed_box\n",
      "..\\ALOHATaskCompressedData\\apply_tape_closed_box\\episode_0.hdf5\n",
      "..\\ALOHATaskCompressedData\\pick_scraper_from_rack\n",
      "..\\ALOHATaskCompressedData\\pick_scraper_from_rack\\episode_0.hdf5\n",
      "..\\ALOHATaskCompressedData\\usb_mark_003\n",
      "..\\ALOHATaskCompressedData\\usb_mark_003\\episode_0.hdf5\n"
     ]
    }
   ],
   "source": [
    "for dataset_folder in sorted(parent_dir.iterdir()):\n",
    "    print(dataset_folder)\n",
    "\n",
    "    # Using natsort to naturally sort the episode files\n",
    "    sorted_files = natsort.natsorted(dataset_folder.glob(\"episode*.hdf5\"))\n",
    "\n",
    "    for hdf5_file in sorted_files:\n",
    "        print(hdf5_file)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eaa1f00-64dd-4298-b94d-6b8e7b803cb3",
   "metadata": {},
   "source": [
    "## Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddef1e44-1f5b-4fa1-8750-94ce98765696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle datasets inside the HDF5 file\n",
    "def extract_data(f, arrays):\n",
    "    for name, obj in f.items():  # Iterate over items in the root group\n",
    "        if isinstance(obj, h5py.Dataset):  # If it's a dataset (not a group)\n",
    "            array = obj[()]  # Extract the data from the dataset\n",
    "            arrays[name] = array  # Store it in the arrays dictionary\n",
    "        elif isinstance(obj, h5py.Group):  # If it's a group, recurse into it\n",
    "            for sub_name, sub_obj in obj.items():\n",
    "                if isinstance(sub_obj, h5py.Dataset):\n",
    "                    array = sub_obj[()]\n",
    "                    arrays[f\"{name}/{sub_name}\"] = array  # Store the data with full path as key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c85dc80c-396f-42b4-9821-2acccecf50e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(f, arrays):\n",
    "    # Iterate over items in the root group (top-level groups and datasets)\n",
    "    for name, obj in f.items():\n",
    "        if isinstance(obj, h5py.Dataset):  # If it's a dataset (not a group)\n",
    "            # Extract data from the dataset\n",
    "            array = obj[()]\n",
    "            arrays[name] = array  # Store it in the arrays dictionary\n",
    "        elif isinstance(obj, h5py.Group):  # If it's a group, recurse into it\n",
    "            for sub_name, sub_obj in obj.items():\n",
    "                if isinstance(sub_obj, h5py.Dataset):  # If it's a dataset inside the group\n",
    "                    # Extract data from the sub-group dataset\n",
    "                    array = sub_obj[()]\n",
    "                    # Use full path (group name + dataset name) as the key in the dictionary\n",
    "                    full_name = f\"{name}/{sub_name}\"\n",
    "                    arrays[full_name] = array  # Store the dataset with full path as key\n",
    "                elif isinstance(sub_obj, h5py.Group):  # If it's a subgroup, recurse further\n",
    "                    # Recursively extract data from nested groups\n",
    "                    extract_data(sub_obj, arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03e8181a-7a6c-4742-9bcb-47ac73160e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dict_keys(arrays):\n",
    "    \n",
    "    old_keys = ['cam_high', 'cam_left_wrist', 'cam_low', 'cam_right_wrist']\n",
    "    updated_arrays = {}\n",
    "\n",
    "    for key, value in arrays.items():\n",
    "        # Check if the current key is in the old_keys list\n",
    "        if key in old_keys:\n",
    "            # Update key with the new path\n",
    "            new_key = f'observations/images/{key}'\n",
    "            updated_arrays[new_key] = value\n",
    "        else:\n",
    "            # Keep the existing key-value pair\n",
    "            updated_arrays[key] = value\n",
    "\n",
    "    return updated_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3aa52d8f-e0c9-488d-a71d-51677a30c0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['action', 'compress_len', 'observations/effort', 'observations/images/cam_high', 'observations/images/cam_left_wrist', 'observations/images/cam_low', 'observations/images/cam_right_wrist', 'observations/qpos', 'observations/qvel'])\n",
      "dict_keys(['action', 'compress_len', 'observations/effort', 'observations/images/cam_high', 'observations/images/cam_left_wrist', 'observations/images/cam_low', 'observations/images/cam_right_wrist', 'observations/qpos', 'observations/qvel'])\n",
      "dict_keys(['action', 'compress_len', 'observations/effort', 'observations/images/cam_high', 'observations/images/cam_left_wrist', 'observations/images/cam_low', 'observations/images/cam_right_wrist', 'observations/qpos', 'observations/qvel'])\n"
     ]
    }
   ],
   "source": [
    "for dataset_folder in sorted(parent_dir.iterdir()):\n",
    "    # Using natsort to naturally sort the episode files\n",
    "    sorted_files = natsort.natsorted(dataset_folder.glob(\"episode*.hdf5\"))\n",
    "\n",
    "    for hdf5_file in sorted_files:\n",
    "        arrays = {}\n",
    "\n",
    "        with h5py.File(hdf5_file, 'r') as f:\n",
    "            extract_data(f, arrays)\n",
    "\n",
    "        arrays = update_dict_keys(arrays)\n",
    "        print(arrays.keys())\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69264ad7-33d1-4800-9dfc-6a658e5bc72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images_to_video(images, out_path, fps=30, is_depth=False):\n",
    "    print(images.shape)\n",
    "    if images.ndim == 4:  # [T, H, W, C]\n",
    "        h, w = images.shape[1:3]\n",
    "    else:  # [T, H, W]\n",
    "        h, w = images.shape[1:3]\n",
    "        images = np.expand_dims(images, -1)\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    writer = cv2.VideoWriter(str(out_path), fourcc, fps, (w, h), isColor=True)\n",
    "\n",
    "    for frame in images:\n",
    "        if is_depth:\n",
    "            frame = np.squeeze(frame)\n",
    "            frame = cv2.normalize(frame, None, 0, 255, cv2.NORM_MINMAX)\n",
    "            frame = frame.astype(np.uint8)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)\n",
    "        else:\n",
    "            frame = frame.astype(np.uint8)\n",
    "        writer.write(frame)\n",
    "\n",
    "    writer.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbb4e8a-9ade-41af-a3a6-7e78eba964a9",
   "metadata": {},
   "source": [
    "## Dataset Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddd4adcf-dade-4756-b418-6858a05a30ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "fps = 30\n",
    "is_depth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7ba2c59-8f00-44e5-b69b-ebedbf50f495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Processing: apply_tape_closed_box\n",
      "📦 Processing episode_0.hdf5\n",
      "🎞️ Saving video: cam_high_rgb_image.rmb.mp4 from key: observations/images/cam_high\n",
      "(800, 23091)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m             video_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcam_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuffix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.rmb.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     42\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🎞️ Saving video: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from key: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 43\u001b[0m             save_images_to_video(arrays[key], rmb_dir \u001b[38;5;241m/\u001b[39m video_name, fps\u001b[38;5;241m=\u001b[39mfps, is_depth\u001b[38;5;241m=\u001b[39mis_depth)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Done: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 6\u001b[0m, in \u001b[0;36msave_images_to_video\u001b[1;34m(images, out_path, fps, is_depth)\u001b[0m\n\u001b[0;32m      4\u001b[0m     h, w \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# [T, H, W]\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     h, w \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m      7\u001b[0m     images \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(images, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      9\u001b[0m fourcc \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoWriter_fourcc(\u001b[38;5;241m*\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmp4v\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "# Process each dataset folder\n",
    "for dataset_folder in sorted(parent_dir.iterdir()):\n",
    "    if dataset_folder.is_dir():  \n",
    "        print(f\"📦 Processing: {dataset_folder.name}\")\n",
    "\n",
    "        # Create necessary output directories specific to this dataset\n",
    "        out_dir = Path(f\"./rmb_dataset/{dataset_folder.name}\")  \n",
    "\n",
    "        # Process each .h5 file in the dataset folder\n",
    "        for i, hdf5_file in enumerate(natsort.natsorted(dataset_folder.glob(\"episode*.hdf5\"))):\n",
    "            print(f\"📦 Processing {hdf5_file.name}\")\n",
    "            episode_name = f\"episode_{i:06d}.rmb\"\n",
    "            rmb_dir = out_dir / episode_name\n",
    "            rmb_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            with h5py.File(hdf5_file, 'r') as f:\n",
    "                arrays = {}\n",
    "                extract_data(f, arrays)\n",
    "                arrays = update_dict_keys(arrays)\n",
    "\n",
    "                obs_keys = [key for key in arrays.keys() if 'observations' in key]\n",
    "                # print(obs_keys)\n",
    "        \n",
    "                # Write non-image data to main.rmb.hdf5\n",
    "                with h5py.File(rmb_dir / \"main.rmb.hdf5\", 'w') as out_f:\n",
    "                    for key, array in arrays.items():\n",
    "                        if 'image' not in key:\n",
    "                            if '/' in key:\n",
    "                                group_name, sub_key = key.split('/', 1)\n",
    "                                group = out_f.require_group(group_name)\n",
    "                                group.create_dataset(sub_key, data=array)\n",
    "                            else:\n",
    "                                out_f.create_dataset(key, data=array)\n",
    "        \n",
    "                # Handle image data\n",
    "                for key in arrays:\n",
    "                    if key.startswith(\"observations/images/\"):\n",
    "                        is_depth = False  # your dataset only has RGB images\n",
    "                        cam_name = key.split(\"/\")[-1]  # e.g., cam_left_wrist\n",
    "                        suffix = \"rgb_image\"\n",
    "                        video_name = f\"{cam_name}_{suffix}.rmb.mp4\"\n",
    "                        print(f\"🎞️ Saving video: {video_name} from key: {key}\")\n",
    "                        save_images_to_video(arrays[key], rmb_dir / video_name, fps=fps, is_depth=is_depth)\n",
    "                        \n",
    "            print(f\"✅ Done: {episode_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec26822a-b38e-4bae-b153-d1917ef6f2c0",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
